{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport nltk\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get treelib\n!pip install -U treelib\n\n# Setup Stanford Parser\n!wget https://nlp.stanford.edu/software/stanford-parser-4.0.0.zip\n!unzip stanford-parser-4.0.0.zip\n!mkdir stanford-nlp-jars\n!cp stanford-parser-4.0.0/stanford-parser.jar stanford-nlp-jars/\n!cp stanford-parser-4.0.0/stanford-parser-4.0.0-models.jar stanford-nlp-jars/\n!unzip stanford-nlp-jars/stanford-parser-4.0.0-models.jar -d stanford-nlp-jars/stanford-parser-4.0.0-models/\n!pip\nfrom nltk.parse import stanford\nos.environ['STANFORD_PARSER'] = 'stanford-nlp-jars'\nos.environ['STANFORD_MODELS'] = 'stanford-nlp-jars'\nconstituency_parser = stanford.StanfordParser(model_path=\"stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\ndependency_parser = stanford.StanfordDependencyParser()\n\n\n# Setup Stanford POS Tagger\n!wget https://nlp.stanford.edu/software/stanford-tagger-4.1.0.zip\n!unzip stanford-tagger-4.1.0.zip\n!cp stanford-postagger-full-2020-08-06/stanford-postagger-4.1.0.jar stanford-nlp-jars/\n!cp stanford-postagger-full-2020-08-06/models/english-bidirectional-distsim.tagger stanford-nlp-jars/\n\nfrom nltk.tag.stanford import StanfordPOSTagger\npos_model = 'stanford-nlp-jars/english-bidirectional-distsim.tagger'\npos_jar   = 'stanford-nlp-jars/stanford-postagger-4.1.0.jar'\npos_tagger = StanfordPOSTagger(pos_model, pos_jar)","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting treelib\n  Downloading treelib-1.6.1.tar.gz (24 kB)\nRequirement already satisfied, skipping upgrade: future in /opt/conda/lib/python3.7/site-packages (from treelib) (0.18.2)\nBuilding wheels for collected packages: treelib\n  Building wheel for treelib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for treelib: filename=treelib-1.6.1-py3-none-any.whl size=18371 sha256=2572b9662fb6e72cd7280cd76818758b8b645b6562e6d7ab94bf1d8608f78d64\n  Stored in directory: /root/.cache/pip/wheels/89/be/94/2c6d949ce599d1443426d83ba4dc93cd35c0f4638260930a53\nSuccessfully built treelib\nInstalling collected packages: treelib\nSuccessfully installed treelib-1.6.1\n--2020-10-31 12:25:21--  https://nlp.stanford.edu/software/stanford-parser-4.0.0.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 182493647 (174M) [application/zip]\nSaving to: ‘stanford-parser-4.0.0.zip’\n\nstanford-parser-4.0 100%[===================>] 174.04M  7.52MB/s    in 19s     \n\n2020-10-31 12:25:41 (9.13 MB/s) - ‘stanford-parser-4.0.0.zip’ saved [182493647/182493647]\n\nArchive:  stanford-parser-4.0.0.zip\n   creating: stanford-parser-4.0.0/\n  inflating: stanford-parser-4.0.0/ShiftReduceDemo.java  \n  inflating: stanford-parser-4.0.0/lexparser-gui.command  \n  inflating: stanford-parser-4.0.0/ejml-core-0.38.jar  \n  inflating: stanford-parser-4.0.0/DependencyParserDemo.java  \n  inflating: stanford-parser-4.0.0/pom.xml  \n  inflating: stanford-parser-4.0.0/ejml-ddense-0.38-sources.jar  \n  inflating: stanford-parser-4.0.0/ejml-simple-0.38-sources.jar  \n  inflating: stanford-parser-4.0.0/lexparser.sh  \n  inflating: stanford-parser-4.0.0/lexparser-gui.bat  \n  inflating: stanford-parser-4.0.0/stanford-parser.jar  \n  inflating: stanford-parser-4.0.0/LICENSE.txt  \n  inflating: stanford-parser-4.0.0/stanford-parser-4.0.0-sources.jar  \n  inflating: stanford-parser-4.0.0/ejml-simple-0.38.jar  \n  inflating: stanford-parser-4.0.0/Makefile  \n  inflating: stanford-parser-4.0.0/lexparser-lang-train-test.sh  \n  inflating: stanford-parser-4.0.0/ParserDemo.java  \n  inflating: stanford-parser-4.0.0/lexparser-lang.sh  \n  inflating: stanford-parser-4.0.0/README.txt  \n  inflating: stanford-parser-4.0.0/stanford-parser-4.0.0-models.jar  \n   creating: stanford-parser-4.0.0/data/\n extracting: stanford-parser-4.0.0/data/english-onesent.txt  \n  inflating: stanford-parser-4.0.0/data/chinese-onesent-unseg-utf8.txt  \n  inflating: stanford-parser-4.0.0/data/testsent.txt  \n  inflating: stanford-parser-4.0.0/data/french-onesent.txt  \n extracting: stanford-parser-4.0.0/data/german-onesent.txt  \n extracting: stanford-parser-4.0.0/data/chinese-onesent-utf8.txt  \n extracting: stanford-parser-4.0.0/data/chinese-onesent-gb18030.txt  \n  inflating: stanford-parser-4.0.0/data/pos-sentences.txt  \n extracting: stanford-parser-4.0.0/data/chinese-onesent-unseg-gb18030.txt  \n extracting: stanford-parser-4.0.0/data/arabic-onesent-utf8.txt  \n   creating: stanford-parser-4.0.0/bin/\n  inflating: stanford-parser-4.0.0/bin/run-tb-preproc  \n  inflating: stanford-parser-4.0.0/bin/makeSerialized.csh  \n  inflating: stanford-parser-4.0.0/lexparser-gui.sh  \n  inflating: stanford-parser-4.0.0/ejml-ddense-0.38.jar  \n   creating: stanford-parser-4.0.0/conf/\n  inflating: stanford-parser-4.0.0/conf/atb-latest.conf  \n  inflating: stanford-parser-4.0.0/conf/ftb-latest.conf  \n  inflating: stanford-parser-4.0.0/slf4j-api-1.7.12-sources.jar  \n  inflating: stanford-parser-4.0.0/lexparser.bat  \n  inflating: stanford-parser-4.0.0/lexparser_lang.def  \n  inflating: stanford-parser-4.0.0/StanfordDependenciesManual.pdf  \n  inflating: stanford-parser-4.0.0/build.xml  \n  inflating: stanford-parser-4.0.0/README_dependencies.txt  \n  inflating: stanford-parser-4.0.0/slf4j-api.jar  \n  inflating: stanford-parser-4.0.0/ejml-core-0.38-sources.jar  \n  inflating: stanford-parser-4.0.0/stanford-parser-4.0.0-javadoc.jar  \n  inflating: stanford-parser-4.0.0/ParserDemo2.java  \nArchive:  stanford-nlp-jars/stanford-parser-4.0.0-models.jar\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/META-INF/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/META-INF/MANIFEST.MF  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/spanishPCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/germanPCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/xinhuaPCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/xinhuaFactored.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/arabicFactored.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/xinhuaFactoredSegmenting.ser.gz  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/UD_French.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/english_UD.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/UD_German.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/UD_Spanish.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/UD_Chinese.gz  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/upos/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/upos/ENUniversalPOS.tsurgeon  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/french-ud.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/english-left3words-distsim.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/spanish-ud.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/chinese-distsim.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/german-ud.tagger  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/ud/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/ud/feature_map.txt  \n\nUsage:   \n  pip <command> [options]\n\nCommands:\n  install                     Install packages.\n  download                    Download packages.\n  uninstall                   Uninstall packages.\n  freeze                      Output installed packages in requirements format.\n  list                        List installed packages.\n  show                        Show information about installed packages.\n  check                       Verify installed packages have compatible dependencies.\n  config                      Manage local and global configuration.\n  search                      Search PyPI for packages.\n  cache                       Inspect and manage pip's wheel cache.\n  wheel                       Build wheels from your requirements.\n  hash                        Compute hashes of package archives.\n  completion                  A helper command used for command completion.\n  debug                       Show information useful for debugging.\n  help                        Show help for commands.\n\nGeneral Options:\n  -h, --help                  Show help.\n  --isolated                  Run pip in an isolated mode, ignoring\n                              environment variables and user configuration.\n  -v, --verbose               Give more output. Option is additive, and can be\n                              used up to 3 times.\n  -V, --version               Show version and exit.\n  -q, --quiet                 Give less output. Option is additive, and can be\n                              used up to 3 times (corresponding to WARNING,\n                              ERROR, and CRITICAL logging levels).\n  --log <path>                Path to a verbose appending log.\n  --no-input                  Disable prompting for input.\n  --proxy <proxy>             Specify a proxy in the form\n                              [user:passwd@]proxy.server:port.\n  --retries <retries>         Maximum number of retries each connection should\n                              attempt (default 5 times).\n  --timeout <sec>             Set the socket timeout (default 15 seconds).\n  --exists-action <action>    Default action when a path already exists:\n                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n                              even though it does not have valid or any HTTPS.\n  --cert <path>               Path to alternate CA bundle.\n  --client-cert <path>        Path to SSL client certificate, a single file\n                              containing the private key and the certificate\n                              in PEM format.\n  --cache-dir <dir>           Store the cache data in <dir>.\n  --no-cache-dir              Disable the cache.\n  --disable-pip-version-check\n                              Don't periodically check PyPI to determine\n                              whether a new version of pip is available for\n                              download. Implied with --no-index.\n  --no-color                  Suppress colored output\n  --no-python-version-warning\n                              Silence deprecation warnings for upcoming\n                              unsupported Pythons.\n  --use-feature <feature>     Enable new functionality, that may be backward\n                              incompatible.\n  --use-deprecated <feature>  Enable deprecated functionality, that will be\n                              removed in the future.\n","name":"stdout"},{"output_type":"stream","text":"--2020-10-31 12:25:50--  https://nlp.stanford.edu/software/stanford-tagger-4.1.0.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 78279880 (75M) [application/zip]\nSaving to: ‘stanford-tagger-4.1.0.zip’\n\nstanford-tagger-4.1 100%[===================>]  74.65M  5.53MB/s    in 9.4s    \n\n2020-10-31 12:26:01 (7.92 MB/s) - ‘stanford-tagger-4.1.0.zip’ saved [78279880/78279880]\n\nArchive:  stanford-tagger-4.1.0.zip\n   creating: stanford-postagger-full-2020-08-06/\n  inflating: stanford-postagger-full-2020-08-06/stanford-postagger-4.1.0-sources.jar  \n  inflating: stanford-postagger-full-2020-08-06/TaggerDemo2.java  \n  inflating: stanford-postagger-full-2020-08-06/TaggerDemo.java  \n  inflating: stanford-postagger-full-2020-08-06/build.xml  \n   creating: stanford-postagger-full-2020-08-06/models/\n  inflating: stanford-postagger-full-2020-08-06/models/arabic.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/spanish-ud.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/english-caseless-left3words-distsim.tagger  \n  inflating: stanford-postagger-full-2020-08-06/models/README-Models.txt  \n  inflating: stanford-postagger-full-2020-08-06/models/arabic-train.tagger  \n  inflating: stanford-postagger-full-2020-08-06/models/english-bidirectional-distsim.tagger  \n  inflating: stanford-postagger-full-2020-08-06/models/english-bidirectional-distsim.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/chinese-distsim.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/chinese-nodistsim.tagger  \n  inflating: stanford-postagger-full-2020-08-06/models/english-left3words-distsim.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/english-caseless-left3words-distsim.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/arabic-train.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/chinese-nodistsim.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/french-ud.tagger  \n  inflating: stanford-postagger-full-2020-08-06/models/german-ud.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/spanish-ud.tagger  \n  inflating: stanford-postagger-full-2020-08-06/models/chinese-distsim.tagger  \n  inflating: stanford-postagger-full-2020-08-06/models/french-ud.tagger.props  \n  inflating: stanford-postagger-full-2020-08-06/models/arabic.tagger  \n  inflating: stanford-postagger-full-2020-08-06/models/english-left3words-distsim.tagger  \n  inflating: stanford-postagger-full-2020-08-06/models/german-ud.tagger  \n  inflating: stanford-postagger-full-2020-08-06/stanford-postagger.jar  \n  inflating: stanford-postagger-full-2020-08-06/sample-output.txt  \n  inflating: stanford-postagger-full-2020-08-06/stanford-postagger.bat  \n  inflating: stanford-postagger-full-2020-08-06/sample-input.txt  \n  inflating: stanford-postagger-full-2020-08-06/stanford-postagger-4.1.0.jar  \n  inflating: stanford-postagger-full-2020-08-06/stanford-postagger-4.1.0-javadoc.jar  \n  inflating: stanford-postagger-full-2020-08-06/stanford-postagger.sh  \n  inflating: stanford-postagger-full-2020-08-06/stanford-postagger-gui.sh  \n  inflating: stanford-postagger-full-2020-08-06/stanford-postagger-gui.bat  \n  inflating: stanford-postagger-full-2020-08-06/README.txt  \n  inflating: stanford-postagger-full-2020-08-06/LICENSE.txt  \n   creating: stanford-postagger-full-2020-08-06/data/\n  inflating: stanford-postagger-full-2020-08-06/data/enclitic-inflections.data  \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Test Parser\nsent1 = \"He teaches my daughter Maths\"\nsent1 = \"The boy who jumped into the river saved another boy\"\nprint(sent1)\n# print(\"Constituency parsing\")\n# for t in constituency_parser.raw_parse(sent1):\n# \tprint(t)\n\nprint(\"Dependency parsing\")\nresult = dependency_parser.raw_parse(sent1)\ndep = result.__next__()\n\nl = list(dep.triples())\nprint(l)\ndep_parse = []\nfor el in l:\n\tdep_parse.append((el[0][0],el[2][0]))\nprint(dep_parse)\n\n# #### Test POS Tagger\ntext = \"He is sincere and honest\"\nwords = nltk.word_tokenize(text)\ntagged_words = pos_tagger.tag(words)\nprint(tagged_words)","execution_count":31,"outputs":[{"output_type":"stream","text":"The boy who jumped into the river saved another boy\nDependency parsing\n[(('saved', 'VBD'), 'nsubj', ('boy', 'NN')), (('boy', 'NN'), 'det', ('The', 'DT')), (('boy', 'NN'), 'acl:relcl', ('jumped', 'VBD')), (('jumped', 'VBD'), 'nsubj', ('who', 'WP')), (('jumped', 'VBD'), 'obl', ('river', 'NN')), (('river', 'NN'), 'case', ('into', 'IN')), (('river', 'NN'), 'det', ('the', 'DT')), (('saved', 'VBD'), 'obj', ('boy', 'NN')), (('boy', 'NN'), 'det', ('another', 'DT'))]\n[('saved', 'boy'), ('boy', 'The'), ('boy', 'jumped'), ('jumped', 'who'), ('jumped', 'river'), ('river', 'into'), ('river', 'the'), ('saved', 'boy'), ('boy', 'another')]\n[('He', 'PRP'), ('is', 'VBZ'), ('sincere', 'JJ'), ('and', 'CC'), ('honest', 'JJ')]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tagdict = nltk.data.load('help/tagsets/upenn_tagset.pickle')\n# print(tagdict.keys())\n# Maps POS Tags\npostagdict = { 'TO':'PREP', 'IN':'PREP',\n        \n        'VB':'VB', 'VBN':'VB', 'VBP':'VB',\n        'VBG':'VB', 'VBZ':'VB', 'VBD':'VB',\n        'MD':'VB',\n        \n        'NNS':'NN', 'NN':'NN', 'NNP':'NN',\n        'NNPS':'NN', 'EX':'NN',\n        \n        'RB':'RB', 'RBR':'RB', 'RBS':'RB',\n        'WRB':'RB',\n        \n        'PRP':'PR', 'PRP$':'PR', 'WP':'PR',\n        'WP$':'PR',\n        \n        \"''\":'OOV', \"UH\":'OOV', 'LS':'OOV',\n        '--':'OOV', ':':'OOV', '(':'OOV',\n        ')':'OOV', '.':'OOV', ',':'OOV',\n        '``':'OOV', '$':'OOV', 'FW':'OOV',\n        'SYM':'OOV',\n        \n        'JJ':'JJ', 'JJR':'JJ', 'JJS':'JJ',\n        \n        'DT':'DT', 'WDT':'DT', 'PDT':'DT',\n        \n        'RP':'CASE',\n        \n        'CC':'CC',\n        \n        'CD':'NUM',\n        \n        'POS':'POS'}\n","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain POS Tags of each word:\n# Number each word from 0 to n-1\nind = 0\ndef traverse_tree_worker(tree_iter):\n    global ind\n    ind = 0\n    for t in tree_iter:\n        new_tree = t\n    pos_tags = []\n    words = []\n    ind = 0\n    \n    \n    def traverse_tree(tree):\n        global ind\n        one_subtree = False\n        extract_label = False\n        for i in range(len(tree)):\n            subtree = tree[i]\n            if type(subtree) == nltk.tree.Tree:\n                one_subtree = True\n                subtree = traverse_tree(subtree)\n            else:\n                words.append(subtree)\n                subtree = ind\n                ind +=1\n            tree[i] = subtree\n\n        if one_subtree == False:\n            pos_tags.append(tree.label())\n        return tree\n    final_tree = traverse_tree(new_tree)\n    iter_tree = iter(final_tree)\n    return iter_tree,pos_tags, words\n\ndef copy_nltk_tree(tree_iter):\n    for t in tree_iter:\n        new_tree = t\n    \n    new_iter = iter(new_tree)\n    return new_iter","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent1 = \"The quick brown fox jumped over the lazy dog\"\nsent1 = \"Senior boys who had exams played football on the ground\"\nsent1 = \"The boy who jumped into the river saved another boy\"\nprint(sent1)\n# print(\"Constituency parsing\")\nparsed_sent1 = constituency_parser.raw_parse(sent1)\nparsed_sent1_old = parsed_sent1\nparsed_sent1_new,pos_tags, words = traverse_tree_worker(parsed_sent1)\nprint(\"Old parse\")\nfor t in parsed_sent1_old:\n\tprint(t)\nprint(\"New parse\")\nfor t in parsed_sent1_new:\n    print(t)","execution_count":34,"outputs":[{"output_type":"stream","text":"The boy who jumped into the river saved another boy\nOld parse\nNew parse\n(S\n  (NP\n    (NP (DT 0) (NN 1))\n    (SBAR\n      (WHNP (WP 2))\n      (S (VP (VBD 3) (PP (IN 4) (NP (DT 5) (NN 6)))))))\n  (VP (VBD 7) (NP (DT 8) (NN 9))))\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def traverse_tree(tree):\n# \tprint(\"tree:\", tree, tree.label())\n# \tfor subtree in tree:\n# \t\tif type(subtree) == nltk.tree.Tree:\n# \t\t\ttraverse_tree(subtree)\n\nsent1 = \"The quick brown fox jumped over the lazy dog\"\nsent1 = \"Senior boys who had exams played football on the ground\"\nsent1 = \"The boy who jumped into the river saved another boy\"\nprint(sent1)\n# print(\"Constituency parsing\")\nparsed_sent1 = constituency_parser.raw_parse(sent1)\nparsed_sent1, pos_tags, words = traverse_tree_worker(parsed_sent1)\n# for t in parsed_sent1:\n# \tprint(t)\n\n\ndef phrase_to_dependency(tree):\n\th,dep_tree = convert_to_dependency_without_root(tree)\n\treturn add_root_node(dep_tree, h)\n\ndef convert_to_dependency_without_root(tree):\n\tchildren_heads = []\n\tdep_tree = []\n\t#print(tree)\n\tfor subtree in tree:\n\t\tif type(subtree) == nltk.tree.Tree and subtree.height() > 2:\n\t\t\th, dep_subtree = convert_to_dependency_without_root(subtree)\n\t\t\tchildren_heads.append(h)\n\t\t\tdep_tree += dep_subtree\n\t\telif type(subtree) == nltk.tree.Tree and subtree.height() == 2:\n\t\t\th = subtree.leaves()[0]#[subtree.label(), subtree.leaves()[0]]\n\t\t\tchildren_heads.append(h)\n\t#print(children_heads)\n\th = find_head(tree)\n\t#print(h)\n\tfor m in children_heads:\n\t\tif m != h:\n\t\t\tdep_tree.append((h, m))\n\t#print(dep_tree)\n\treturn h, dep_tree\n\ndef add_root_node(dep_tree, h):\n\tdep_tree.append((-1, h))\n\treturn dep_tree\n\ndef find_head_old(possible_heads):\n\th = possible_heads[0]\n\tfor p in possible_heads:\n\t\tif more_imp(p[0],h[0]):\n\t\t\th = p\n\t#print(h)\n\treturn h\n\ndef find_head(tree):\n\tif tree.label() == 'NP':\n\t\treturn find_head_of_np(tree)\n\telif tree.label() == 'VP':\n\t\treturn find_head_of_vp(tree)\n\t# elif tree.label() == 'PP':\n\t# \tfind_head_of_pp(tree)\n\telif tree.label() == 'ROOT':\n\t\tsubtrees = [t for t in tree if type(t)==nltk.tree.Tree]\n\t\treturn find_head_of_s(subtrees[0])\n\telif tree.label() == 'S':\n\t\treturn find_head_of_s(tree)\n\telse:\n\t\treturn find_head_in_general(tree)\n\ndef find_head_of_np(np):\n\tnoun_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']\n\ttop_level_trees = [np[i] for i in range(len(np)) if type(np[i]) is nltk.tree.Tree]\n\t## search for a top-level noun\n\ttop_level_nouns = [t.leaves()[0] for t in top_level_trees if t.label() in noun_tags]\n\tif len(top_level_nouns) > 0:\n\t\treturn top_level_nouns[-1]\n\telse:\n\t\ttop_level_nps = [t for t in top_level_trees if t.label()=='NP']\n\t\tif len(top_level_nps) > 0:\n\t\t\treturn find_head_of_np(top_level_nps[-1])\n\t\telse:\n\t\t\tnouns = [p[0] for p in np.pos() if p[1] in noun_tags]\n\t\t\tif len(nouns) > 0:\n\t\t\t\treturn nouns[-1]\n\t\t\telse:\n\t\t\t\treturn np.leaves()[-1]\n\ndef find_head_of_vp(vp):\n\tverb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n\ttop_level_trees = [vp[i] for i in range(len(vp)) if type(vp[i]) is nltk.tree.Tree]\n\ttop_level_verbs = [t.leaves()[0] for t in top_level_trees if t.label() in verb_tags]\n\tif len(top_level_verbs) > 0:\n\t\treturn top_level_verbs[0]\n\telse:\n\t\ttop_level_vps = [t for t in top_level_trees if t.label()=='VP']\n\t\tif len(top_level_vps) > 0:\n\t\t\treturn find_head_of_vp(top_level_vps[0])\n\t\telse:\n\t\t\tverbs = [p[0] for p in vp.pos() if p[1] in verb_tags]\n\t\t\tif len(verbs) > 0:\n\t\t\t\treturn verbs[0]\n\t\t\telse:\n\t\t\t\treturn vp.leaves()[0]\n\ndef find_head_of_s(s):\n\ttop_level_trees = [s[i] for i in range(len(s)) if type(s[i]) is nltk.tree.Tree]\n\ttop_level_vps = [t for t in top_level_trees if t.label()=='VP']\n\ttop_level_nps = [t for t in top_level_trees if t.label()=='NP']\n\tif len(top_level_vps) > 0:\n\t\treturn find_head_of_vp(top_level_vps[0])\n\telif len(top_level_nps) > 0:\n\t\treturn find_head_of_np(top_level_nps[-1])\n\telse:\n\t\treturn s.leaves()[0]\n\ndef find_head_in_general(s):\n\ttop_level_trees = [s[i] for i in range(len(s)) if type(s[i]) is nltk.tree.Tree]\n\ttop_level_vps = [t for t in top_level_trees if t.label()=='VP']\n\ttop_level_nps = [t for t in top_level_trees if t.label()=='NP']\n\ttop_level_s = [t for t in top_level_trees if t.label()=='S']\n\tif len(top_level_vps) > 0:\n\t\treturn find_head_of_vp(top_level_vps[0])\n\telif len(top_level_nps) > 0:\n\t\treturn find_head_of_np(top_level_nps[-1])\n\telif len(top_level_s) > 0:\n\t\treturn find_head_of_s(top_level_s[0])\t\n\telse:\n\t\treturn s.leaves()[0]\n\n## Not required\n# def find_head_of_pp(pp):\n# \tprep_tags = ['TO', 'IN']\n# \ttop_level_trees = [pp[i] for i in range(len(pp)) if type(pp[i]) is nltk.tree.Tree]\n# \ttop_level_preps = [t.leaves()[0] for t in top_level_trees if t.label() in prep_tags]\n# \tif len(top_level_preps) > 0:\n# \t\treturn top_level_preps[0]\n# \telse:\n# \t\ttop_level_pps = [t for t in top_level_trees if t.label()=='PP']\n# \t\tif len(top_level_pps) > 0:\n# \t\t\treturn find_head_of_pp(top_level_pps[0])\n# \t\telse:\n# \t\t\tpreps = [p[0] for p in pp.pos() if p[1] in prep_tags]\n# \t\t\tif len(preps) > 0:\n# \t\t\t\treturn preps[0]\n# \t\t\telse:\n# \t\t\t\treturn pp.leaves()[0]\t\n\ndef more_imp(t1, t2):\n\tif t1.startswith(\"V\") and not t2.startswith(\"V\"):\n\t\treturn True\n\tif t1.startswith(\"N\") and not (t2.startswith(\"V\") or t2.startswith(\"N\")):\n\t\treturn True\n\n\treturn False\n\nfor t in parsed_sent1:\n\tprint(\"Dependency parser\")\n\tdep_tree = phrase_to_dependency(t)\n\tprint(dep_tree)","execution_count":35,"outputs":[{"output_type":"stream","text":"The boy who jumped into the river saved another boy\nDependency parser\n[(1, 0), (6, 5), (6, 4), (3, 6), (3, 2), (1, 3), (9, 8), (7, 9), (7, 1), (-1, 7)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from treelib import Tree, Node\ndef printDependencyTree(lst, sent):\n    '''\n        Input: A list with values as (parent, child, tag)[Note: Indices]\n               Sentence\n        Prints the tree\n    '''\n    words_with_index = []\n    for i in range(len(lst)):\n        words_with_index.append(\"Dummy\")\n        \n    i = 0\n    for item in lst:\n        new_word = f\"{item[2]}-{sent[item[1]]}-{item[1]}\"\n        i+=1\n        words_with_index[item[1]] = new_word\n    \n    tree = Tree()\n    tree.create_node(\"Root\", -1)\n    for (t1, t2, t3) in lst:\n        tree.create_node(words_with_index[t2], t2, parent = -1)\n    for (t1, t2, t3) in lst:\n        tree.move_node(t2, t1)\n    tree.show()\n    \nsent = ['The', 'boy', 'jumped']\n# lst = [(-1, 2), (2, 1), (1, 0)]\nlst  = [(-1, 2, 'VB'), (2, 1, 'NSUBJ'), (1, 0, 'DET')]\nprintDependencyTree(lst, sent)","execution_count":47,"outputs":[{"output_type":"stream","text":"Root\n└── VB-jumped-2\n    └── NSUBJ-boy-1\n        └── DET-The-0\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"printDependencyTree(dep_tree, words_with_index)","execution_count":37,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"tuple index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-c82fbb80a037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprintDependencyTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_with_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-36-990ed4bf923f>\u001b[0m in \u001b[0;36mprintDependencyTree\u001b[0;34m(lst, sent)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mnew_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{item[2]}-{sent[item[1]]}-{i}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mwords_with_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"children = []\nfor i in range(len(words)):\n    children.append([])\n\nfor word_pair in new_dep_tree:\n    parent = word_pair[0]\n    child = word_pair[1]\n    if parent != -1:\n        children[parent].append(child)\nprint(children)","execution_count":38,"outputs":[{"output_type":"stream","text":"[[], [0, 3], [], [6, 2], [], [], [5, 4], [9, 1], [], [8]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def addRules(dependency_tree,children,pos_tags):\n    \n    dep_tree_with_tags = []\n    \n    # Get main verb in the sentence\n    main_verb = 0\n    parent_cc = 0\n    for words in dependency_tree:\n        if words[0] == -1:\n            main_verb = words[1]\n        if postagdict[pos_tags[words[1]]] == 'CC':\n            parent_cc = words[0]\n    \n    # Find the last noun after main verb\n    last_noun = 0\n    for i in range(main_verb, len(children)):\n        if postagdict[pos_tags[i]] == 'NN':\n            last_noun = i\n    \n    # Start tagging edges\n    for words in dependency_tree:\n        new_grouping = []\n\n        new_grouping.append(words[0])\n        new_grouping.append(words[1])\n        pos = postagdict[pos_tags[words[1]]] \n        if words[0] != -1:\n            pos_parent = postagdict[pos_tags[words[0]]]\n        else:\n            pos_parent = \"ROOT\"\n            new_grouping.append(\"ROOT\")\n            dep_tree_with_tags.append(new_grouping)\n            continue\n            \n        if pos == \"DT\":\n            new_grouping.append(\"DET\")\n        \n        elif pos == \"CC\":\n            new_grouping.append(\"CC\")\n        \n        elif pos_parent == \"NN\" and pos == \"JJ\":\n            new_grouping.append(\"AMOD\")\n        \n        elif pos_parent == \"NN\" and pos == \"NUM\":\n            new_grouping.append(\"NUMMOD\")\n        \n        elif pos_parent == \"NN\" and pos == \"NN\":\n            # Resolve APPOS or NMOD\n            isAppos = True\n            for i in range(words[0], words[1]+1):\n                if postagdict[pos_tags[i]] == \"PREP\":\n                    isAppos = False\n                    break\n            if isAppos is True:\n                new_grouping.append(\"APPOS\")\n            else:\n                new_grouping.append(\"NMOD\")\n        \n        elif pos_parent == \"VB\" and pos == \"NN\":\n            ## Resolve Object and Subject\n            if words[0] > words[1]:\n                new_grouping.append(\"NSUBJ\")\n            elif words[1] == last_noun:\n                new_grouping.append(\"DOBJ\")\n            else:\n                new_grouping.append(\"IOBJ\")\n        \n        elif pos_parent == \"VB\" and (pos == \"VB\" or pos == \"JJ\"):\n            new_grouping.append(\"CCOMP\")\n        \n        elif pos != \"VB\" and pos != \"NN\" and pos != \"JJ\" and pos != \"DT\":\n            new_grouping.append(\"CASE\")\n        \n        elif words[1] == parent_cc:\n            new_grouping.append(\"CONJ\")\n        \n        else:\n            new_grouping.append(\"XCOMP\")\n        dep_tree_with_tags.append(new_grouping)\n    return dep_tree_with_tags\n\n\ndep_tree_with_tags = addRules(dep_tree, children, pos_tags)\nprint(dep_tree_with_tags)\nprintDependencyTree(dep_tree_with_tags, words)\n                \n            ","execution_count":48,"outputs":[{"output_type":"stream","text":"[[1, 0, 'DET'], [6, 5, 'DET'], [6, 4, 'CASE'], [3, 6, 'IOBJ'], [3, 2, 'CASE'], [1, 3, 'XCOMP'], [9, 8, 'DET'], [7, 9, 'DOBJ'], [7, 1, 'NSUBJ'], [-1, 7, 'ROOT']]\nRoot\n└── ROOT-saved-7\n    ├── DOBJ-boy-9\n    │   └── DET-another-8\n    └── NSUBJ-boy-1\n        ├── DET-The-0\n        └── XCOMP-jumped-3\n            ├── CASE-who-2\n            └── IOBJ-river-6\n                ├── CASE-into-4\n                └── DET-the-5\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"[[1, 0, 'DET'],\n [6, 5, 'DET'],\n [6, 4, 'OTH'],\n [3, 6, 'DOBJ'],\n [3, 2, 'OTH'],\n [1, 3, 'OTH'],\n [9, 8, 'DET'],\n [7, 9, 'DOBJ'],\n [7, 1, 'NSUBJ']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dep_tree_with_tags)","execution_count":28,"outputs":[{"output_type":"stream","text":"[[1, 0, 'DET'], [6, 5, 'DET'], [6, 4, 'CASE'], [3, 6, 'IOBJ'], [3, 2, 'CASE'], [1, 3, 'XCOMP'], [9, 8, 'DET'], [7, 9, 'DOBJ'], [7, 1, 'NSUBJ'], [-1, 7, 'ROOT']]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(words[7])","execution_count":41,"outputs":[{"output_type":"stream","text":"saved\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}