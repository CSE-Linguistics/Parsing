{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport nltk\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get treelib\n!pip install -U treelib\n\n# Setup Stanford Parser\n!wget https://nlp.stanford.edu/software/stanford-parser-4.0.0.zip\n!unzip stanford-parser-4.0.0.zip\n!mkdir stanford-nlp-jars\n!cp stanford-parser-4.0.0/stanford-parser.jar stanford-nlp-jars/\n!cp stanford-parser-4.0.0/stanford-parser-4.0.0-models.jar stanford-nlp-jars/\n!unzip stanford-nlp-jars/stanford-parser-4.0.0-models.jar -d stanford-nlp-jars/stanford-parser-4.0.0-models/\n!pip\nfrom nltk.parse import stanford\nos.environ['STANFORD_PARSER'] = 'stanford-nlp-jars'\nos.environ['STANFORD_MODELS'] = 'stanford-nlp-jars'\nconstituency_parser = stanford.StanfordParser(model_path=\"stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\ndependency_parser = stanford.StanfordDependencyParser()\n\n\n# # Setup Stanford POS Tagger: Not needed\n# !wget https://nlp.stanford.edu/software/stanford-tagger-4.1.0.zip\n# !unzip stanford-tagger-4.1.0.zip\n# !cp stanford-postagger-full-2020-08-06/stanford-postagger-4.1.0.jar stanford-nlp-jars/\n# !cp stanford-postagger-full-2020-08-06/models/english-bidirectional-distsim.tagger stanford-nlp-jars/\n\n# from nltk.tag.stanford import StanfordPOSTagger\n# pos_model = 'stanford-nlp-jars/english-bidirectional-distsim.tagger'\n# pos_jar   = 'stanford-nlp-jars/stanford-postagger-4.1.0.jar'\n# pos_tagger = StanfordPOSTagger(pos_model, pos_jar)","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting treelib\n  Downloading treelib-1.6.1.tar.gz (24 kB)\nRequirement already satisfied, skipping upgrade: future in /opt/conda/lib/python3.7/site-packages (from treelib) (0.18.2)\nBuilding wheels for collected packages: treelib\n  Building wheel for treelib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for treelib: filename=treelib-1.6.1-py3-none-any.whl size=18371 sha256=87ec7abb1908ea63d719cdd2edeb29d6c586cad84d298b93063dad1ec090ee5e\n  Stored in directory: /root/.cache/pip/wheels/89/be/94/2c6d949ce599d1443426d83ba4dc93cd35c0f4638260930a53\nSuccessfully built treelib\nInstalling collected packages: treelib\nSuccessfully installed treelib-1.6.1\n--2020-11-01 13:21:55--  https://nlp.stanford.edu/software/stanford-parser-4.0.0.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 182493647 (174M) [application/zip]\nSaving to: ‘stanford-parser-4.0.0.zip’\n\nstanford-parser-4.0 100%[===================>] 174.04M  25.3MB/s    in 6.8s    \n\n2020-11-01 13:22:02 (25.7 MB/s) - ‘stanford-parser-4.0.0.zip’ saved [182493647/182493647]\n\nArchive:  stanford-parser-4.0.0.zip\n   creating: stanford-parser-4.0.0/\n  inflating: stanford-parser-4.0.0/ShiftReduceDemo.java  \n  inflating: stanford-parser-4.0.0/lexparser-gui.command  \n  inflating: stanford-parser-4.0.0/ejml-core-0.38.jar  \n  inflating: stanford-parser-4.0.0/DependencyParserDemo.java  \n  inflating: stanford-parser-4.0.0/pom.xml  \n  inflating: stanford-parser-4.0.0/ejml-ddense-0.38-sources.jar  \n  inflating: stanford-parser-4.0.0/ejml-simple-0.38-sources.jar  \n  inflating: stanford-parser-4.0.0/lexparser.sh  \n  inflating: stanford-parser-4.0.0/lexparser-gui.bat  \n  inflating: stanford-parser-4.0.0/stanford-parser.jar  \n  inflating: stanford-parser-4.0.0/LICENSE.txt  \n  inflating: stanford-parser-4.0.0/stanford-parser-4.0.0-sources.jar  \n  inflating: stanford-parser-4.0.0/ejml-simple-0.38.jar  \n  inflating: stanford-parser-4.0.0/Makefile  \n  inflating: stanford-parser-4.0.0/lexparser-lang-train-test.sh  \n  inflating: stanford-parser-4.0.0/ParserDemo.java  \n  inflating: stanford-parser-4.0.0/lexparser-lang.sh  \n  inflating: stanford-parser-4.0.0/README.txt  \n  inflating: stanford-parser-4.0.0/stanford-parser-4.0.0-models.jar  \n   creating: stanford-parser-4.0.0/data/\n extracting: stanford-parser-4.0.0/data/english-onesent.txt  \n  inflating: stanford-parser-4.0.0/data/chinese-onesent-unseg-utf8.txt  \n  inflating: stanford-parser-4.0.0/data/testsent.txt  \n  inflating: stanford-parser-4.0.0/data/french-onesent.txt  \n extracting: stanford-parser-4.0.0/data/german-onesent.txt  \n extracting: stanford-parser-4.0.0/data/chinese-onesent-utf8.txt  \n extracting: stanford-parser-4.0.0/data/chinese-onesent-gb18030.txt  \n  inflating: stanford-parser-4.0.0/data/pos-sentences.txt  \n extracting: stanford-parser-4.0.0/data/chinese-onesent-unseg-gb18030.txt  \n extracting: stanford-parser-4.0.0/data/arabic-onesent-utf8.txt  \n   creating: stanford-parser-4.0.0/bin/\n  inflating: stanford-parser-4.0.0/bin/run-tb-preproc  \n  inflating: stanford-parser-4.0.0/bin/makeSerialized.csh  \n  inflating: stanford-parser-4.0.0/lexparser-gui.sh  \n  inflating: stanford-parser-4.0.0/ejml-ddense-0.38.jar  \n   creating: stanford-parser-4.0.0/conf/\n  inflating: stanford-parser-4.0.0/conf/atb-latest.conf  \n  inflating: stanford-parser-4.0.0/conf/ftb-latest.conf  \n  inflating: stanford-parser-4.0.0/slf4j-api-1.7.12-sources.jar  \n  inflating: stanford-parser-4.0.0/lexparser.bat  \n  inflating: stanford-parser-4.0.0/lexparser_lang.def  \n  inflating: stanford-parser-4.0.0/StanfordDependenciesManual.pdf  \n  inflating: stanford-parser-4.0.0/build.xml  \n  inflating: stanford-parser-4.0.0/README_dependencies.txt  \n  inflating: stanford-parser-4.0.0/slf4j-api.jar  \n  inflating: stanford-parser-4.0.0/ejml-core-0.38-sources.jar  \n  inflating: stanford-parser-4.0.0/stanford-parser-4.0.0-javadoc.jar  \n  inflating: stanford-parser-4.0.0/ParserDemo2.java  \nArchive:  stanford-nlp-jars/stanford-parser-4.0.0-models.jar\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/META-INF/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/META-INF/MANIFEST.MF  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/spanishPCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/germanPCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/xinhuaPCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/xinhuaFactored.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/arabicFactored.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/lexparser/xinhuaFactoredSegmenting.ser.gz  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/\n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/UD_French.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/english_UD.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/UD_German.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/UD_Spanish.gz  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/parser/nndep/UD_Chinese.gz  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/upos/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/upos/ENUniversalPOS.tsurgeon  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/french-ud.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/english-left3words-distsim.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/spanish-ud.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/chinese-distsim.tagger  \n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/pos-tagger/german-ud.tagger  \n   creating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/ud/\n  inflating: stanford-nlp-jars/stanford-parser-4.0.0-models/edu/stanford/nlp/models/ud/feature_map.txt  \n\nUsage:   \n  pip <command> [options]\n\nCommands:\n  install                     Install packages.\n  download                    Download packages.\n  uninstall                   Uninstall packages.\n  freeze                      Output installed packages in requirements format.\n  list                        List installed packages.\n  show                        Show information about installed packages.\n  check                       Verify installed packages have compatible dependencies.\n  config                      Manage local and global configuration.\n  search                      Search PyPI for packages.\n  cache                       Inspect and manage pip's wheel cache.\n  wheel                       Build wheels from your requirements.\n  hash                        Compute hashes of package archives.\n  completion                  A helper command used for command completion.\n  debug                       Show information useful for debugging.\n  help                        Show help for commands.\n\nGeneral Options:\n  -h, --help                  Show help.\n  --isolated                  Run pip in an isolated mode, ignoring\n                              environment variables and user configuration.\n  -v, --verbose               Give more output. Option is additive, and can be\n                              used up to 3 times.\n  -V, --version               Show version and exit.\n  -q, --quiet                 Give less output. Option is additive, and can be\n                              used up to 3 times (corresponding to WARNING,\n                              ERROR, and CRITICAL logging levels).\n  --log <path>                Path to a verbose appending log.\n  --no-input                  Disable prompting for input.\n  --proxy <proxy>             Specify a proxy in the form\n                              [user:passwd@]proxy.server:port.\n  --retries <retries>         Maximum number of retries each connection should\n                              attempt (default 5 times).\n  --timeout <sec>             Set the socket timeout (default 15 seconds).\n  --exists-action <action>    Default action when a path already exists:\n                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n                              even though it does not have valid or any HTTPS.\n  --cert <path>               Path to alternate CA bundle.\n  --client-cert <path>        Path to SSL client certificate, a single file\n                              containing the private key and the certificate\n                              in PEM format.\n  --cache-dir <dir>           Store the cache data in <dir>.\n  --no-cache-dir              Disable the cache.\n  --disable-pip-version-check\n                              Don't periodically check PyPI to determine\n                              whether a new version of pip is available for\n                              download. Implied with --no-index.\n  --no-color                  Suppress colored output\n  --no-python-version-warning\n                              Silence deprecation warnings for upcoming\n                              unsupported Pythons.\n  --use-feature <feature>     Enable new functionality, that may be backward\n                              incompatible.\n  --use-deprecated <feature>  Enable deprecated functionality, that will be\n                              removed in the future.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Test Parser\nsent1 = \"The boy who jumped into the river saved another boy\"\n# sent1 = \"The boy who jumped into the river saved another boy\"\nprint(sent1)\nprint(\"Constituency parsing\")\nfor t in constituency_parser.raw_parse(sent1):\n\tt.pretty_print()\n\nprint(\"Dependency parsing\")\nresult = dependency_parser.raw_parse(sent1)\ndep = result.__next__()\n\nl = list(dep.triples())\nprint(l)\ndep_parse = []\nfor el in l:\n\tdep_parse.append((el[0][0],el[2][0]))\nprint(dep_parse)\n\n# #### Test POS Tagger\n# text = \"He is sincere and honest\"\n# words = nltk.word_tokenize(text)\n# tagged_words = pos_tagger.tag(words)\n# print(tagged_words)","execution_count":3,"outputs":[{"output_type":"stream","text":"The boy who jumped into the river saved another boy\nConstituency parsing\n                        ROOT                                        \n                         |                                           \n                         S                                          \n                    _____|______________________________             \n                   NP                                   |           \n      _____________|_____                               |            \n     |                  SBAR                            |           \n     |        ___________|____                          |            \n     |       |                S                         |           \n     |       |                |                         |            \n     |       |                VP                        |           \n     |       |      __________|___                      |            \n     |       |     |              PP                    VP          \n     |       |     |      ________|___            ______|_____       \n     NP     WHNP   |     |            NP         |            NP    \n  ___|___    |     |     |         ___|____      |       _____|___   \n DT      NN  WP   VBD    IN       DT       NN   VBD     DT        NN\n |       |   |     |     |        |        |     |      |         |  \nThe     boy who  jumped into     the     river saved another     boy\n\nDependency parsing\n[(('saved', 'VBD'), 'nsubj', ('boy', 'NN')), (('boy', 'NN'), 'det', ('The', 'DT')), (('boy', 'NN'), 'acl:relcl', ('jumped', 'VBD')), (('jumped', 'VBD'), 'nsubj', ('who', 'WP')), (('jumped', 'VBD'), 'obl', ('river', 'NN')), (('river', 'NN'), 'case', ('into', 'IN')), (('river', 'NN'), 'det', ('the', 'DT')), (('saved', 'VBD'), 'obj', ('boy', 'NN')), (('boy', 'NN'), 'det', ('another', 'DT'))]\n[('saved', 'boy'), ('boy', 'The'), ('boy', 'jumped'), ('jumped', 'who'), ('jumped', 'river'), ('river', 'into'), ('river', 'the'), ('saved', 'boy'), ('boy', 'another')]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generateConstituencyTree(lst):\n    string = \"(\"\n    for i in range(len(lst)):\n        if isinstance(lst[i], list):\n            string = string + \" \" + generateConstituencyTree(lst[i]) + \" \"\n        else:\n            string = string + \" \" + lst[i] + \" \"\n    string = string + \")\"\n    return string","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"85c8b21d-efec-4b77-b481-bed0ad2650b9","_cell_guid":"f8b1c0fa-bad8-44c6-9356-c8c466ebbeb8","trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom itertools import chain\n\ndef dependency_to_phrase(tree):\n\tindex = 0\n\tchildren = list(tree.nodes[index][\"deps\"].values())\n\tphrase_tree = []\n\tfor c in children:\n\t\tfor idx in c:\n\t\t\tphrase_tree.append(convert_to_phrase_without_root(tree, idx))\n\tphrase_tree = ['ROOT'] + phrase_tree\n\treturn phrase_tree\n\n\ndef convert_to_phrase_without_root(tree, index):\n\tnoun_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']\n\tverb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n\tchildren = list(tree.nodes[index][\"deps\"].values())\n\tif len(children) == 0:\n\t\tif tree.nodes[index][\"tag\"] not in noun_tags:\n\t\t\treturn [tree.nodes[index][\"tag\"], tree.nodes[index][\"word\"]]\n\t\telse:\n\t\t\treturn [[tree.nodes[index][\"tag\"], tree.nodes[index][\"word\"]]]\n\n\tright_children = []\n\tleft_children = []\n\tfor l in children:\n\t\tfor idx in l:\n\t\t\tif idx > index:\n\t\t\t\tright_children.append(idx)\n\t\t\telif idx < index:\n\t\t\t\tleft_children.append(idx)\n\n\tleft_children.reverse()\n\tphrase_tree = [[tree.nodes[index][\"tag\"], tree.nodes[index][\"word\"]]]\n\tleft_ext_projections = []\n\tright_ext_projections = []\n\t# print(right_children, left_children)\n\tfor lc in left_children:\n\t\tphrase_subtree = convert_to_phrase_without_root(tree, lc)\n\t\tif  (tree.nodes[lc][\"tag\"] in noun_tags and tree.nodes[index][\"tag\"] in verb_tags):\n\t\t\tleft_ext_projections = [phrase_subtree] + left_ext_projections\n\t\telse:\n\t\t\tphrase_tree = [phrase_subtree] + phrase_tree\n\tfor rc in right_children:\n\t\tphrase_subtree = convert_to_phrase_without_root(tree, rc)\n\t\tif  (tree.nodes[rc][\"tag\"] in verb_tags and tree.nodes[index][\"tag\"] in noun_tags):\n\t\t\tright_ext_projections.append(phrase_subtree)\n\t\telse:\n\t\t\tphrase_tree.append(phrase_subtree)\n\tif tree.nodes[index][\"tag\"] in noun_tags:\n\t\tphrase_tree = ['NP'] + phrase_tree\n\telif tree.nodes[index][\"tag\"] in verb_tags:\n\t\tphrase_tree = ['VP'] + phrase_tree\n\telse:\n\t\tphrase_tree = ['X'] + phrase_tree\n\tif len(left_ext_projections) + len(right_ext_projections) > 0:\n\t\tphrase_tree = left_ext_projections + [phrase_tree] + right_ext_projections\n\t\tif len(left_ext_projections) > 0:\n\t\t\tphrase_tree = ['S'] + phrase_tree\n\t\telse:\n\t\t\tphrase_tree = ['NP'] + phrase_tree\n\treturn phrase_tree\n\n\nsent1 = \"The boy who jumped into the river saved another boy\"\nparsed_sent1 = dependency_parser.raw_parse(sent1)\nfor t in parsed_sent1:\n    lst = dependency_to_phrase(t)\n    x = nltk.tree.Tree.fromstring(generateConstituencyTree(lst))\n    x.pretty_print()","execution_count":5,"outputs":[{"output_type":"stream","text":"                       ROOT                                \n                        |                                   \n                        S                                  \n                   _____|______________________             \n                  NP                           |           \n      ____________|_____                       |            \n     |                  VP                     VP          \n     |        __________|____            ______|_____       \n     NP      |    |          NP         |            NP    \n  ___|___    |    |      ____|____      |       _____|___   \n DT      NN  WP  VBD    IN   DT   NN   VBD     DT        NN\n |       |   |    |     |    |    |     |      |         |  \nThe     boy who jumped into the river saved another     boy\n\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}